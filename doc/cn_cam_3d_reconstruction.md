# 图片恢复场景点云

这是一份报告文档，说明基于单目深度估计技术，用序列图片信息恢复场景点云的流程。同时提出一些潜在问题，解决思路以及未来展望。

**单目深度估计**粗略的定义是指用模型推理RGB图像中每个像素的（绝对或相对）深度值（后称深度图像）。

获得深度图像后，可使用相机内参，将相机平面上的每个像素点反投影到相机坐标系上，得到单帧点云。利用相机的外参，拼接每一帧点云，得到场景的点云。

---

## 数据采集
* 有的单目深度估计的[工作](https://arxiv.org/abs/1904.04998)仅需要一个相机的图像序列就能得到每帧图像的深度图像（同时得到相机内参，外参）。但这类工作生成的深度图像是相对深度，为了能接近达到项目上线的标准，本项目还使用了相机的内参，组合导航的pose。
* 从rosbag导出数据时，需同步组合导航和相机（前置相机，如lingang_map5的cam00）的时间戳，使每一帧图片对应的外参信息尽可能保持一致。注意，组合导航获得的外参信息并不一定是相机的外参信息，可能需要进行坐标变换。
* 本项目测试Autowise数据集时，相机导出的图片频率是10Hz。也可参考KITTI，CityScape等公开数据集。

## 训练模型
### 模型选择
这个项目最初是希望用[MonoRec](https://github.com/Brummi/MonoRec)作为单目深度估计的模型，复现结果时发现MonoRec对输入数据要求比较高，故改用同年另一个组产出的工作[Manydepth](https://github.com/nianticlabs/manydepth)。实践中发现Manydepth是基于该组之前的工作[Monodepth2](https://github.com/nianticlabs/monodepth2)，且在Autowise数据集上并没有比Monodepth2的效果更好，故最终选择结构更为简单的Monodepth2。除了Monodepth2以外，[PackNet](https://github.com/TRI-ML/packnet-sfm)表现也不错，但代码框架封装的比较深，不好理解。

### 数据准备
* 对采集的图片进行去畸变，裁减。注意，去畸变和裁减这两个过程可能会导致相机内参变化，具体可看[此处](https://www.cnblogs.com/Todd-Qi/p/13149270.html)。裁减天空对深度预测值影响不大，但可降低显卡内存使用。
* 单目深度估计的一个基本假设是相机移动，因此学术界普遍会使用去除了相机不动样本的splits（如KITTI的Eigen Zhou splits)。针对非公开数据集，可考虑使用组合导航信息或图片的光流幅值，去除车静止的帧。
* 单目深度估计还有一个基本假设是场景静止，因此可以将拍摄到的移动的物体mask掉。可考虑用mask-rcnn进行实例分割，筛选出面积较大的实例，然后检测面积较大的实例的光流值，去除光流值比较小的实例。
* 单目深度估计模型对图片边缘畸变处的深度估计比较差，采用畸变较小的相机获得的图片效果会好一些。

### 模型训练
* Monodepth2作为一个自监督且是单目的模型，预测出来的深度是相对深度（丢失尺度），本项目在实践中，参考PackNet的velocity loss，发现translation loss（用组合导航帧间位移的模做监督）不需要考虑坐标系转换就可以解决尺度的问题。

* 训练时，有时会出现陷入局部最优的情况，表现为tensorboard上展示的深度图像几乎全为0。这种情况可尝试重新训练，或参考Manydepth引入random seed。

* 数据增强对指标提升很重要。但由于内参固定，外参也不能很方便地随着数据增强而更新，因此数据增强的形式只限于颜色变化。不能使用裁切，缩放，旋转等破坏内参，外参的形式。

* 使用gps或组合导航提供的外参（转换到相机坐标系上）替代PoseNet，对训练结果**没有**显著的影响。

* 动态物体除了可能会产生深度值错误的空洞之外，还会影响地面深度值的估计，比如被超车时，图片下方出现车的若干行地面的深度值会发生很大变化。

* 训练时，使用其他模型预训练得到的动态物体Mask，对上述地面深度值变化的问题**没有**改善。


## 恢复点云
### 数据准备
* 单目深度估计模型中一般会有PoseNet预测帧间的相对Pose。如果没有其他方式获得Pose，可以在模型推理时，保存这个信息。这个相对Pose是在相机坐标系上的，因此在累积成每一帧相机的外参时不需要做坐标变换。
* 无论是使用PoseNet还是其他方式，在相机产生较大旋转的地方得到的相机外参都有很大的偏差，使得车在转弯时拼接点云效果会很差。为了缓解这个问题，可考虑降低图像在汽车直行时的采样频率，使相机在旋转时产生的变化相对没那么大。

### 恢复单帧点云
* 单目深度估计模型在图像边缘畸变点，较远的像素点上预测出的深度并不准确，因此需要将这类点移除。可考虑将深度值大于某个阈值的点删除。
* 单目深度估计得到的深度图像目前仅能恢复出点云的大体结构，大部分精细的结构并不准确，因此将它们当作outlier去除是比较好的选择。

### 拼接多帧点云
* 拼接前先检查单帧点云的尺度是否与pose相匹配。
* 多帧点云累积过程中会有重合部分，使用voxelFilter之类的filter可以去除重复部分，以减小点云尺寸。
* 尝试过用组合导航的pose（已转换到相机坐标系下）作为初值，使用ICP或NDT优化帧间点云的相对pose，效果不如初值（可能是代码有问题）。

## 评价指标
常规的单目深度估计评价指标针对预测的深度图像与GT。而实际上像素级别深度图像的GT并不容易获得，因此评价非公开数据集深度估计效果在这一步上就卡住了。另外在常规深度估计评价指标上比较好的模型，预测出的深度图像在用于场景重建中也并没有得到很好的效果。我认为这套评价指标对于当前深度估计过于严苛，意义不够明确。更合理的方法可能是将预测出的深度用以某一特定的更好评价的下游任务，然后再通过评价这个下游任务的指标去评价预测出的深度的质量（如评价反投影的点云和雷达反投影出的点云的相似度），这引出下面的方向展望。

---

## 问题与方向展望
自监督单目深度估计这套方法（基于SfMLearner改进，如Monodepth2，MonoRec，PackNet，[Google's work](https://arxiv.org/abs/2010.16404)等）目前并不能在保证时效的条件下，得到可以直接用于下游任务（如反投影成点云）的深度图像。其他的即时深度估计方案，如用图像信息补全雷达稀疏深度([depth completion](https://arxiv.org/abs/2103.16690))也并不能得到准确率高的深度图像。

除了将深度图像用于重建场经典云，深度图像还可以用于三维检测，三维分割等任务。如果这些下游任务完全相信深度信息是可靠的，将会产生很大的误差影响，如图像畸变部分重建出来的点云不可用。目前在这一块能做的改善的仅是让下游任务从预测出来的深度图像中找出正确的数据，如重建点云时将深度较大的点删除。

这里的核心问题是，对于下游任务，只有准确率非常高的深度图像才有意义。而这样的深度图像一般只有在获得了三维模型后才能通过渲染技术获得。

于是一个很自然的想法是，比起用于产生深度图像这一中间产物，不如将这类模型改造成多任务模型，如[Insta-DM](https://github.com/SeokjuLee/Insta-DM)；也可以作为一个子模块，嵌入到多模态多任务的模型，如融合Lidar信息和image信息进行BEV分割任务[BEVFusion](https://bevfusion.mit.edu/)。直接作用于下游任务，可以更好地利用模型端到端推理的优势。

而关于线下图片恢复场景点云这一需求，抛开单目深度估计这一路线，还可以尝试使用[NeRF](https://www.matthewtancik.com/nerf)衍生的模型，投喂若干张图像及对应外参，利用differential rendering拟合物理模型的隐式表达，再将模型转化为为点云形式。具体可见[Block-NeRF](https://waymo.com/intl/zh-cn/research/block-nerf/)，[instant-ngp](https://github.com/NVlabs/instant-ngp)。